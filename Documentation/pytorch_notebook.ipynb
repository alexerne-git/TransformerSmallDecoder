{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small notebook that follows the structure of the code and defines examples of different functions and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Input Tokens, Input Embeddings, Positional Embeddings:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Broadcasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1 (shape: torch.Size([2, 3, 4])):\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "Tensor 2 (shape: torch.Size([3, 4])):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Result (shape: torch.Size([2, 3, 4])):\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "# Simple Broadcasting Example: https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "\n",
    "# Broadcasting # (B,T,C) + (T,C) = (B,T,C) # (2,3,4) + (3,4) = (2,3,4)\n",
    "# Define a tensor of shape (2, 3, 4)\n",
    "tensor1 = torch.ones((2, 3, 4))\n",
    "# Define a tensor of shape (3, 4)\n",
    "tensor2 = torch.arange(12).reshape((3, 4))\n",
    "# Broadcasting addition\n",
    "result = tensor1 + tensor2\n",
    "print(\"Tensor 1 (shape: {}):\\n{}\".format(tensor1.shape, tensor1))\n",
    "print(\"Tensor 2 (shape: {}):\\n{}\".format(tensor2.shape, tensor2))\n",
    "print(\"Result (shape: {}):\\n{}\".format(result.shape, result))\n",
    "# Last 2 dimensions should be of same size for broadcasting to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **nn.Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor (shape: torch.Size([1, 4, 5])):\n",
      "tensor([[[ 0.2523,  1.9565, -0.0244,  0.1356,  1.4870],\n",
      "         [ 0.2593, -0.2504, -0.7831, -1.6453,  1.7529],\n",
      "         [ 0.6701,  0.3365, -0.1757,  1.5942,  0.3505],\n",
      "         [ 0.5761, -0.7605, -0.5727, -0.9205,  0.1978]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Firt Element of Embedding Table:\n",
      "tensor([ 0.5761, -0.7605, -0.5727, -0.9205,  0.1978],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "-\n",
      "Input tensor (shape: torch.Size([2, 4])):\n",
      "tensor([[1, 2, 3, 0],\n",
      "        [1, 3, 2, 1]])\n",
      "Output tensor (shape: torch.Size([2, 4, 5])):\n",
      "tensor([[[ 0.2523,  1.9565, -0.0244,  0.1356,  1.4870],\n",
      "         [ 0.2593, -0.2504, -0.7831, -1.6453,  1.7529],\n",
      "         [ 0.6701,  0.3365, -0.1757,  1.5942,  0.3505],\n",
      "         [ 0.5761, -0.7605, -0.5727, -0.9205,  0.1978]],\n",
      "\n",
      "        [[ 0.2523,  1.9565, -0.0244,  0.1356,  1.4870],\n",
      "         [ 0.6701,  0.3365, -0.1757,  1.5942,  0.3505],\n",
      "         [ 0.2593, -0.2504, -0.7831, -1.6453,  1.7529],\n",
      "         [ 0.2523,  1.9565, -0.0244,  0.1356,  1.4870]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Embedding for first element:\n",
      "tensor([ 0.2523,  1.9565, -0.0244,  0.1356,  1.4870],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "# Small example of creating an embedding lookup table for a B,T (B=1) tensor and a B,T (B=2) tensor\n",
    "\n",
    "# Define simple lookup embedding table\n",
    "embedding = torch.nn.Embedding(num_embeddings=4, embedding_dim=5)\n",
    "\n",
    "# Testing with tensor of shape B,T = 1,4\n",
    "input = torch.tensor([[1, 2, 3, 0]])\n",
    "output = embedding(input)\n",
    "print(\"Output tensor (shape: {}):\\n{}\".format(output.shape, output))\n",
    "print(\"Firt Element of Embedding Table:\\n{}\".format(embedding.weight[0]))\n",
    "print(\"-\")\n",
    "# Testing with a tensor of shape B,T = 2,4\n",
    "input = torch.tensor([[1, 2, 3, 0],[1, 3, 2, 1]], dtype=torch.long)\n",
    "output = embedding(input)\n",
    "print(\"Input tensor (shape: {}):\\n{}\".format(input.shape, input))\n",
    "# Lookup embeddings\n",
    "output = embedding(input)\n",
    "print(\"Output tensor (shape: {}):\\n{}\".format(output.shape, output))\n",
    "# Get the embedding for the first element\n",
    "print(\"Embedding for first element:\\n{}\".format(output[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([3, 2, 1, 0])\n",
      "Tensor shape: torch.Size([4])\n",
      "Tensor 2:\n",
      "tensor([0, 1, 2, 3])\n",
      "-\n",
      "Tensor:\n",
      "tensor([[ 1.2848e-02,  1.1640e-02, -8.8220e-03],\n",
      "        [-1.7866e-02, -4.3383e-03, -4.9922e-03],\n",
      "        [-1.1544e-03,  1.4105e-02,  8.3314e-06]])\n",
      "-\n",
      "Selected Values:\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "Tensor Softmax:\n",
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "-\n",
      "Tensor 2D Softmax:\n",
      "tensor([[0.0040, 0.9742, 0.0218],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "-\n",
      "Tensor shape:\n",
      "torch.Size([1, 4, 6])\n",
      "Tensor:\n",
      "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16., 17., 18.],\n",
      "         [19., 20., 21., 22., 23., 24.]]])\n",
      "Last token logits:\n",
      "tensor([[19., 20., 21., 22., 23., 24.]])\n",
      "shaoe: torch.Size([1, 6])\n",
      "Last token softmax:\n",
      "tensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n"
     ]
    }
   ],
   "source": [
    "# Other small operations\n",
    "\n",
    "# Torch Arrange \n",
    "tensor = torch.tensor([3,2,1,0])\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "print(\"Tensor shape: {}\".format(tensor.shape))\n",
    "tensor_2 = torch.arange(len(tensor))\n",
    "print(\"Tensor 2:\\n{}\".format(tensor_2))\n",
    "print(\"-\")\n",
    "\n",
    "# Initialize from normal distribution\n",
    "tensor = nn.init.normal_(torch.empty(3, 3),mean=0.0,std=0.01)\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "print(\"-\")\n",
    "\n",
    "# Sample from multinomial distribution (iterate 10 times to see different values)\n",
    "selected_values = []\n",
    "for i in range(10):\n",
    "    tensor = torch.tensor([0.75,0.20,0.05])\n",
    "    new_val = torch.multinomial(tensor, 1)\n",
    "    selected_values.append(new_val.item())\n",
    "print(\"Selected Values:\\n{}\".format(selected_values))\n",
    "\n",
    "# Compute softmax\n",
    "tensor_unormalized = torch.tensor([1.0, 2.0, 3.0])\n",
    "tensor_softmax = torch.nn.functional.softmax(tensor_unormalized, dim=-1) # -1 or 0 = last dimension\n",
    "print(\"Tensor Softmax:\\n{}\".format(tensor_softmax))\n",
    "print(\"-\")\n",
    "tensor_2D_unormalized = torch.tensor([[-1.5,4,0.2],[1.0, 2.0, 3.0]])\n",
    "tensor_2D_softmax = torch.nn.functional.softmax(tensor_2D_unormalized, dim=-1) # -1 or 0 = last dimension\n",
    "print(\"Tensor 2D Softmax:\\n{}\".format(tensor_2D_softmax))\n",
    "print(\"-\")\n",
    "\n",
    "# Logits of last token (B, T, C) = (1, 4, 6)\n",
    "# Define a tensor of shape (1, 4, 6)\n",
    "tensor = torch.tensor([[\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],  # Token 1\n",
    "    [7.0, 8.0, 9.0, 10.0, 11.0, 12.0],  # Token 2\n",
    "    [13.0, 14.0, 15.0, 16.0, 17.0, 18.0],  # Token 3\n",
    "    [19.0, 20.0, 21.0, 22.0, 23.0, 24.0]   # Token 4 (last token)\n",
    "]])\n",
    "\n",
    "print(\"Tensor shape:\\n{}\".format(tensor.shape))\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "# Logits of the last token\n",
    "last_token_logits = tensor[:, -1, :]  # Select logits of the last token in the sequence\n",
    "print(\"Last token logits:\\n{}\".format(last_token_logits))\n",
    "print(\"shaoe: {}\".format(last_token_logits.shape))\n",
    "# Softmax of the last token\n",
    "last_token_softmax = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "print(\"Last token softmax:\\n{}\".format(last_token_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Table:\n",
      "Parameter containing:\n",
      "tensor([[-0.3997,  0.6892, -0.9752,  0.1737, -0.1289],\n",
      "        [-0.2019,  2.3380, -2.0946, -0.5255, -1.4160],\n",
      "        [-1.1896, -0.4828,  0.9657, -0.3822,  0.0784],\n",
      "        [ 0.6606, -2.8634, -0.0859, -1.1614,  0.1609]], requires_grad=True)\n",
      "First Element of Embedding Table:\n",
      "tensor([-0.3997,  0.6892, -0.9752,  0.1737, -0.1289],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output Tensor (Test 1):\n",
      "tensor([[[-0.2019,  2.3380, -2.0946, -0.5255, -1.4160],\n",
      "         [-1.1896, -0.4828,  0.9657, -0.3822,  0.0784],\n",
      "         [ 0.6606, -2.8634, -0.0859, -1.1614,  0.1609],\n",
      "         [-0.3997,  0.6892, -0.9752,  0.1737, -0.1289]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "--------------------------------------------------\n",
      "Input Tensor (Test 2):\n",
      "tensor([[1, 1, 2, 3],\n",
      "        [1, 2, 3, 0]])\n",
      "Output Tensor (Test 2):\n",
      "tensor([[[-0.2019,  2.3380, -2.0946, -0.5255, -1.4160],\n",
      "         [-0.2019,  2.3380, -2.0946, -0.5255, -1.4160],\n",
      "         [-1.1896, -0.4828,  0.9657, -0.3822,  0.0784],\n",
      "         [ 0.6606, -2.8634, -0.0859, -1.1614,  0.1609]],\n",
      "\n",
      "        [[-0.2019,  2.3380, -2.0946, -0.5255, -1.4160],\n",
      "         [-1.1896, -0.4828,  0.9657, -0.3822,  0.0784],\n",
      "         [ 0.6606, -2.8634, -0.0859, -1.1614,  0.1609],\n",
      "         [-0.3997,  0.6892, -0.9752,  0.1737, -0.1289]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Embedding for First Element in Output Tensor (Test 2):\n",
      "tensor([-0.2019,  2.3380, -2.0946, -0.5255, -1.4160],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the embedding layer with a fixed number of embeddings and dimension\n",
    "embedding = nn.Embedding(num_embeddings=4, embedding_dim=5)\n",
    "\n",
    "# Test 1: Input tensor of shape (1, 4)\n",
    "input1 = torch.tensor([[1, 2, 3, 0]], dtype=torch.long)\n",
    "output1 = embedding(input1)\n",
    "\n",
    "print(\"Embedding Table:\\n{}\".format(embedding.weight))\n",
    "print(\"First Element of Embedding Table:\\n{}\".format(embedding.weight[0]))\n",
    "print(\"Output Tensor (Test 1):\\n{}\".format(output1))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test 2: Input tensor of shape (2, 4) using the same embedding layer\n",
    "input2 = torch.tensor([[1, 1, 2, 3], [1, 2, 3, 0]], dtype=torch.long)\n",
    "output2 = embedding(input2)\n",
    "\n",
    "print(\"Input Tensor (Test 2):\\n{}\".format(input2))\n",
    "print(\"Output Tensor (Test 2):\\n{}\".format(output2))\n",
    "print(\"Embedding for First Element in Output Tensor (Test 2):\\n{}\".format(output2[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.4902,  0.7087, -0.7820, -0.1957, -1.3458,  2.9447],\n",
      "        [-0.6739, -0.2622,  1.8307, -0.6616, -0.1938,  0.0837],\n",
      "        [-0.0206,  0.0453, -1.2263, -0.2533,  0.5648,  0.5291],\n",
      "        [-0.6192,  1.9398, -1.5451,  1.5133, -1.5949, -1.0607],\n",
      "        [-0.9507,  0.6232,  0.3598,  0.7308,  0.2165, -0.0649],\n",
      "        [ 1.0993, -0.0993,  0.4505,  1.1188, -0.6635, -0.9021],\n",
      "        [ 1.4747, -0.4545, -0.7924, -0.8058,  1.5112, -0.1523]],\n",
      "       requires_grad=True)\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 5]])\n",
      "tensor([[[-0.6739, -0.2622,  1.8307, -0.6616, -0.1938,  0.0837],\n",
      "         [-0.0206,  0.0453, -1.2263, -0.2533,  0.5648,  0.5291],\n",
      "         [-0.6192,  1.9398, -1.5451,  1.5133, -1.5949, -1.0607],\n",
      "         [-0.9507,  0.6232,  0.3598,  0.7308,  0.2165, -0.0649]],\n",
      "\n",
      "        [[-0.6739, -0.2622,  1.8307, -0.6616, -0.1938,  0.0837],\n",
      "         [-0.0206,  0.0453, -1.2263, -0.2533,  0.5648,  0.5291],\n",
      "         [-0.6192,  1.9398, -1.5451,  1.5133, -1.5949, -1.0607],\n",
      "         [ 1.0993, -0.0993,  0.4505,  1.1188, -0.6635, -0.9021]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# Create a simple embedding matrix\n",
    "vocab_size = 7 # number of words in the vocabulary\n",
    "embedding_dim = 6 # dimension of the word embeddings\n",
    "embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "print(embedding.weight)\n",
    "\n",
    "# Create a tensor of Batch Size x Sequence Length (Batch Size = 2, Sequence Length = 3)\n",
    "tokenized_text = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 5]])\n",
    "print(tokenized_text)\n",
    "# Get the embeddings for the tensor\n",
    "embeddings = embedding(tokenized_text)\n",
    "print(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1124,  1.3414, -0.0276],\n",
      "        [ 0.9247,  0.1645, -1.0012],\n",
      "        [ 0.3607,  0.7374, -0.5141],\n",
      "        [-0.3547, -0.0061, -1.5211]], requires_grad=True)\n",
      "--\n",
      "Parameter containing:\n",
      "tensor([[-1.2013,  0.9507, -1.5531],\n",
      "        [ 1.0079,  1.0736,  0.6594],\n",
      "        [ 0.5344,  0.2102,  0.3869]], requires_grad=True)\n",
      "Inputs \n",
      " tensor([[0, 2, 1],\n",
      "        [0, 1, 2]])\n",
      "--\n",
      "Input Embeddings \n",
      " tensor([[[-0.1124,  1.3414, -0.0276],\n",
      "         [ 0.3607,  0.7374, -0.5141],\n",
      "         [ 0.9247,  0.1645, -1.0012]],\n",
      "\n",
      "        [[-0.1124,  1.3414, -0.0276],\n",
      "         [ 0.9247,  0.1645, -1.0012],\n",
      "         [ 0.3607,  0.7374, -0.5141]]], grad_fn=<EmbeddingBackward0>)\n",
      "--\n",
      "tensor([0, 1, 2])\n",
      "Positional embeddings \n",
      " tensor([[-1.2013,  0.9507, -1.5531],\n",
      "        [ 1.0079,  1.0736,  0.6594],\n",
      "        [ 0.5344,  0.2102,  0.3869]], grad_fn=<EmbeddingBackward0>)\n",
      "--\n",
      "Final embeddings \n",
      " tensor([[[-1.3137,  2.2921, -1.5807],\n",
      "         [ 1.3685,  1.8110,  0.1453],\n",
      "         [ 1.4591,  0.3748, -0.6143]],\n",
      "\n",
      "        [[-1.3137,  2.2921, -1.5807],\n",
      "         [ 1.9326,  1.2381, -0.3419],\n",
      "         [ 0.8951,  0.9477, -0.1271]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting simplified example\n",
    "vocab_size = 4\n",
    "embedding_dim = 3\n",
    "block_size = 3\n",
    "batch_size = 2\n",
    "\n",
    "embedding_table = torch.nn.Embedding(vocab_size,embedding_dim) \n",
    "print(embedding_table.weight)\n",
    "print(\"--\")\n",
    "positional_embedding_table = torch.nn.Embedding(block_size,embedding_dim) \n",
    "print(positional_embedding_table.weight)\n",
    "\n",
    "# B,T tensor of shape (batch_size, block_size)\n",
    "input_ids = torch.tensor([[0, 2, 1],[0, 1, 2]])\n",
    "print('Inputs \\n',input_ids)\n",
    "print(\"--\")\n",
    "# B,T tensor of shape (batch_size, block_size, embedding_dim)\n",
    "input_embeddings = embedding_table(input_ids)\n",
    "print('Input Embeddings \\n', input_embeddings)\n",
    "print(\"--\")\n",
    "# B,T tensor of shape (batch_size, block_size, embedding_dim)\n",
    "print(torch.arange(block_size))\n",
    "positional_embeddings = positional_embedding_table(torch.arange(block_size))\n",
    "print('Positional embeddings \\n',positional_embeddings)\n",
    "print(\"--\")\n",
    "# B,T tensor of shape (batch_size, block_size, embedding_dim)\n",
    "embeddings = input_embeddings + positional_embeddings\n",
    "print('Final embeddings \\n',embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1. Masked Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Linear Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3037,  0.0434,  0.0251, -0.1345],\n",
      "        [ 0.4486,  0.2598,  0.3083, -0.3898]], requires_grad=True)\n",
      "--\n",
      "Input Embeddings \n",
      " tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]]])\n",
      "--\n",
      "Output \n",
      " tensor([[[-0.6792,  0.3340],\n",
      "         [-2.1575,  2.8416],\n",
      "         [-3.6358,  5.3492]]], grad_fn=<UnsafeViewBackward0>) torch.Size([1, 3, 2])\n",
      "Output \n",
      " tensor([[[-0.6792,  0.3340],\n",
      "         [-2.1575,  2.8416],\n",
      "         [-3.6358,  5.3492]]], grad_fn=<UnsafeViewBackward0>) torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Simple Linear Layer Example # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "n_embed = 4\n",
    "head_size = 2\n",
    "# W is stored as a matrix of shape (out_features, in_features) but the linear layer expects it to be (in_features, out_features)\n",
    "linear_layer = torch.nn.Linear(in_features=n_embed, out_features=head_size,bias=False)\n",
    "print(linear_layer.weight)\n",
    "print(\"--\")\n",
    "# B,T tensor of shape (batch_size, block_size, n_embed) = (1,3,4)\n",
    "input_embeddings = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]]])\n",
    "print('Input Embeddings \\n', input_embeddings)\n",
    "print(\"--\")\n",
    "output = linear_layer(input_embeddings)\n",
    "print('Output \\n', output, output.shape)\n",
    "\n",
    "# Rewrite using the formula: output = input * weight^T\n",
    "output = torch.matmul(input_embeddings, linear_layer.weight.t())\n",
    "print('Output \\n', output, output.shape)\n",
    "\n",
    "# Comment: pytorch function handles direclty the transpose of the weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Attention Score Formula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 3, 5],\n",
      "        [2, 4, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Few transpose\n",
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(a)\n",
    "print(a.transpose(-2, -1)) # -2 and -1 are the last two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "tensor([[[0.4630, 0.1789],\n",
      "         [0.1553, 0.8568],\n",
      "         [0.8870, 0.0808]]])\n",
      "tensor([[[1.5053, 1.1166],\n",
      "         [1.5053, 1.1166]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "4\n",
      "tensor([[[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.ones((1, 2, 3)) # Shape (1, 2, 2)\n",
    "print(q)\n",
    "k = torch.rand((1, 2, 3)).transpose(-2,-1) # Shape (1,3, 2) # Transpose the last two dimensions\n",
    "print(k)\n",
    "wei = q @ k\n",
    "print(wei)\n",
    "# Output: (1, 2, 2) = (1, 2, 3) @ (1, 3, 2)\n",
    "# Output[0][0][0] = q[0][0] @ k[0][0] = q[0][0][0] * k[0][0][0] + q[0][0][1] * k[0][0][1] + q[0][0][2] * k[0][0][2]\n",
    "\n",
    "# Sqrt by shape k\n",
    "k = torch.ones((1, 2, 4)) # Shape (1, 2, 4)\n",
    "print(k)\n",
    "print(k.shape[-1])\n",
    "k = k / k.shape[-1] ** 0.5\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Masking**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Mask:\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Masked Tensor:\n",
      "tensor([[1., -inf, -inf],\n",
      "        [1., 1., -inf],\n",
      "        [1., 1., 1.]])\n",
      "Softmax Tensor:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [-inf, 1., 1.],\n",
      "        [-inf, -inf, 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Lower triangular masl\n",
    "# Create a tensor of shape (3, 3)\n",
    "tensor = torch.ones((3, 3))\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "# Create a mask using tril function\n",
    "mask = torch.tril(torch.ones((3, 3)))\n",
    "print(\"Mask:\\n{}\".format(mask))\n",
    "# Apply the mask to the tensor\n",
    "masked_tensor = tensor.masked_fill(mask == 0, float('-inf'))\n",
    "print(\"Masked Tensor:\\n{}\".format(masked_tensor))\n",
    "# Apply softmax to the masked tensor\n",
    "softmax_tensor = torch.nn.functional.softmax(masked_tensor, dim=-1)\n",
    "print(\"Softmax Tensor:\\n{}\".format(softmax_tensor)) # this computes row-wise softmax on last dimension\n",
    "\n",
    "# Create a upper triangular mask\n",
    "tensor = torch.ones((3, 3))\n",
    "mask = torch.triu(torch.ones((3, 3)))\n",
    "masked_tensor = tensor.masked_fill(mask == 0, float('-inf'))\n",
    "print(masked_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2. Norm and Add (Pre-Norm)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape:\n",
      "torch.Size([1, 3, 4])\n",
      "Tensor:\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]]])\n",
      "Output shape:\n",
      "torch.Size([1, 3, 4])\n",
      "Output:\n",
      "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "# Say: B,T,C = 1,3,4\n",
    "# Define a tensor of shape (1, 3, 4)\n",
    "tensor = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]]])\n",
    "print(\"Tensor shape:\\n{}\".format(tensor.shape))\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "# Layer normalization\n",
    "layer_norm = torch.nn.LayerNorm(normalized_shape=4)\n",
    "output = layer_norm(tensor)\n",
    "print(\"Output shape:\\n{}\".format(output.shape))\n",
    "print(\"Output:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized token 1:\n",
      "tensor([-1.3416, -0.4472,  0.4472,  1.3416])\n",
      "tensor([1., 2., 3., 4.])\n",
      "Original token: tensor([1., 2., 3., 4.])\n",
      "Normalized token: tensor([-1.3416, -0.4472,  0.4472,  1.3416])\n"
     ]
    }
   ],
   "source": [
    "# This is done for each token in the sequence\n",
    "# Print the normalized token of the first sequence\n",
    "print(\"Normalized token 1:\\n{}\".format(output[0][0]))\n",
    "# If we compute manually the normalized token 1\n",
    "single_token = tensor[0][0]\n",
    "print(single_token)\n",
    "# Compute mean and std for the last dimension of the single token\n",
    "eps = 1e-5\n",
    "mean = single_token.mean(dim=-1, keepdim=True)\n",
    "std = single_token.std(dim=-1, unbiased=False, keepdim=True)\n",
    "# Normalize the single token\n",
    "normalized_token = (single_token - mean) / (std + eps)\n",
    "print(\"Original token:\", single_token)\n",
    "print(\"Normalized token:\", normalized_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:\n",
      "torch.Size([1, 3, 4])\n",
      "Output:\n",
      "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]]])\n"
     ]
    }
   ],
   "source": [
    "# Manually writing the formula of Pytorch LayerNorm\n",
    "def layer_norm(tensor, eps=1e-5):\n",
    "    mean = tensor.mean(dim=-1, keepdim=True)\n",
    "    std = tensor.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    return (tensor - mean) / (std + eps)\n",
    "\n",
    "output = layer_norm(tensor)\n",
    "print(\"Output shape:\\n{}\".format(output.shape))\n",
    "print(\"Output:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape:\n",
      "torch.Size([1, 3, 4])\n",
      "Tensor:\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]]])\n",
      "Output shape (LayerNorm):\n",
      "torch.Size([1, 3, 4])\n",
      "Output (LayerNorm):\n",
      "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Output shape (BatchNorm):\n",
      "torch.Size([1, 3, 4])\n",
      "Output (BatchNorm):\n",
      "tensor([[[-1.2247, -1.2247, -1.2247, -1.2247],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.2247,  1.2247,  1.2247,  1.2247]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compare batch normalization and layer normalization\n",
    "tensor = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]]])\n",
    "print(\"Tensor shape:\\n{}\".format(tensor.shape))\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "\n",
    "# Layer normalization # Normalize each token independently across its embedding dimensions\n",
    "layer_norm = torch.nn.LayerNorm(normalized_shape=4)\n",
    "output_layer_norm = layer_norm(tensor)\n",
    "print(\"Output shape (LayerNorm):\\n{}\".format(output_layer_norm.shape))\n",
    "print(\"Output (LayerNorm):\\n{}\".format(output_layer_norm))\n",
    "\n",
    "# Batch normalization # Normalize each embedding dimension independently across all tokens\n",
    "batch_norm = torch.nn.BatchNorm1d(num_features=4)\n",
    "output_batch_norm = batch_norm(tensor.view(-1, 4)).view(tensor.shape)\n",
    "print(\"Output shape (BatchNorm):\\n{}\".format(output_batch_norm.shape))\n",
    "print(\"Output (BatchNorm):\\n{}\".format(output_batch_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3. Feed-Forward Neural-Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([[-1.,  0.,  1.],\n",
      "        [ 2., -2.,  0.]])\n",
      "Output:\n",
      "tensor([[0., 0., 1.],\n",
      "        [2., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Relu Activation Function (zeroes out negative values)\n",
    "tensor = torch.tensor([[-1.0, 0.0, 1.0], [2.0, -2.0, 0.0]])\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "relu = torch.nn.ReLU()\n",
    "output = relu(tensor)\n",
    "print(\"Output:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean of gradients without ReLU: -0.002167\n",
      "Average mean of gradients with ReLU: 0.001124\n"
     ]
    }
   ],
   "source": [
    "# Function to compute mean gradients with optional ReLU\n",
    "def compute_gradients(with_relu, runs=10):\n",
    "    mean_grads = []\n",
    "    for _ in range(runs):\n",
    "        # Define the input tensor\n",
    "        tensor = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "\n",
    "        # Define a 2-layer feedforward network\n",
    "        layers = [\n",
    "            torch.nn.Linear(in_features=3, out_features=128),  # First layer\n",
    "        ]\n",
    "        if with_relu:\n",
    "            layers.append(torch.nn.ReLU())  # Add ReLU activation if specified\n",
    "        layers.append(torch.nn.Linear(in_features=128, out_features=3))  # Second layer\n",
    "        feedforward = torch.nn.Sequential(*layers)\n",
    "\n",
    "        # Apply the feedforward network to the tensor\n",
    "        output = feedforward(tensor)\n",
    "\n",
    "        # Compute gradients\n",
    "        loss = output.sum()  # Simple loss function\n",
    "        loss.backward()  # Backpropagation\n",
    "\n",
    "        # Record the mean of the gradients of the input tensor\n",
    "        mean_grads.append(tensor.grad.mean().item())\n",
    "\n",
    "    # Return the average of the mean gradients over multiple runs\n",
    "    return sum(mean_grads) / runs\n",
    "\n",
    "# Experiment: Compare gradients with and without ReLU\n",
    "mean_grad_without_relu = compute_gradients(with_relu=False, runs=10000)\n",
    "mean_grad_with_relu = compute_gradients(with_relu=True, runs=10000)\n",
    "\n",
    "# Print results\n",
    "print(\"Average mean of gradients without ReLU: {:.6f}\".format(mean_grad_without_relu))\n",
    "print(\"Average mean of gradients with ReLU: {:.6f}\".format(mean_grad_with_relu))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Linear Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor (shape: torch.Size([1, 3, 4])):\n",
      "tensor([[[-0.6495,  0.2708,  0.9177, -1.7845],\n",
      "         [-0.6495,  0.2708,  0.9177, -1.7845],\n",
      "         [-0.6495,  0.2708,  0.9177, -1.7845]]], grad_fn=<EmbeddingBackward0>)\n",
      "Logits tensor (shape: torch.Size([1, 3, 5])):\n",
      "tensor([[[-0.4887,  1.1743,  0.5417,  0.0886,  0.7914],\n",
      "         [-0.4887,  1.1743,  0.5417,  0.0886,  0.7914],\n",
      "         [-0.4887,  1.1743,  0.5417,  0.0886,  0.7914]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Last Linear Layer to produce logits\n",
    "# Define the last linear layer\n",
    "B,T,C = 1,3,4\n",
    "vocab_size = 5\n",
    "# Embedding matrix\n",
    "embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=C)\n",
    "input = torch.ones((B, T), dtype=torch.long)\n",
    "output = embedding(input)\n",
    "print(\"Output tensor (shape: {}):\\n{}\".format(output.shape, output))\n",
    "\n",
    "# Define the last linear layer # (B,T,vocab_size)\n",
    "linear = torch.nn.Linear(in_features=C, out_features=vocab_size)\n",
    "logits = linear(output)\n",
    "print(\"Logits tensor (shape: {}):\\n{}\".format(logits.shape, logits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Training After Linear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "2.3170299530029297\n",
      "Softmax:\n",
      "tensor([[0.6590, 0.2424, 0.0986]])\n",
      "Negative Log Likelihood:\n",
      "2.3170299530029297\n"
     ]
    }
   ],
   "source": [
    "# Small example cross entropy loss\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "# 1D tensor\n",
    "# Define the target tensor\n",
    "target = torch.tensor([2])\n",
    "# Define the predicted logits tensor\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "# Define the cross-entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, target)\n",
    "print(\"Loss:\\n{}\".format(loss))\n",
    "\n",
    "# Now if we compute manually\n",
    "# Compute the softmax of the logits\n",
    "softmax = torch.nn.functional.softmax(logits, dim=-1)\n",
    "print(\"Softmax:\\n{}\".format(softmax))\n",
    "# Compute the negative log likelihood of the target\n",
    "nll = -torch.log(softmax[0][target.item()])\n",
    "print(\"Negative Log Likelihood:\\n{}\".format(nll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor (shape: torch.Size([2, 3, 4])):\n",
      "tensor([[[2.1440, 0.4149, 0.5095, 0.1742],\n",
      "         [2.1440, 0.4149, 0.5095, 0.1742],\n",
      "         [2.1440, 0.4149, 0.5095, 0.1742]],\n",
      "\n",
      "        [[2.1440, 0.4149, 0.5095, 0.1742],\n",
      "         [2.1440, 0.4149, 0.5095, 0.1742],\n",
      "         [2.1440, 0.4149, 0.5095, 0.1742]]], grad_fn=<EmbeddingBackward0>)\n",
      "Flattened output tensor (shape: torch.Size([6, 4])):\n",
      "tensor([[2.1440, 0.4149, 0.5095, 0.1742],\n",
      "        [2.1440, 0.4149, 0.5095, 0.1742],\n",
      "        [2.1440, 0.4149, 0.5095, 0.1742],\n",
      "        [2.1440, 0.4149, 0.5095, 0.1742],\n",
      "        [2.1440, 0.4149, 0.5095, 0.1742],\n",
      "        [2.1440, 0.4149, 0.5095, 0.1742]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# B,T,vocab_size\n",
    "B,T,vocab_size = 2,3,5\n",
    "# Embedding matrix of shape (B,T,vocab_size)\n",
    "embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=C)\n",
    "input = torch.ones((B, T), dtype=torch.long)\n",
    "output = embedding(input)\n",
    "print(\"Output tensor (shape: {}):\\n{}\".format(output.shape, output))\n",
    "\n",
    "# Flatten the output tensor\n",
    "# Flatten the output tensor to shape (B*T, vocab_size)\n",
    "output_flattened = output.view(B*T, -1)\n",
    "print(\"Flattened output tensor (shape: {}):\\n{}\".format(output_flattened.shape, output_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape (B*T, vocab_size):\n",
      "torch.Size([8, 7])\n",
      "Logits:\n",
      "tensor([[ 0.7507, -1.6571, -1.8844,  1.9095,  1.3594,  1.9325,  0.0358],\n",
      "        [ 0.0491, -0.3235,  0.8121,  0.0802, -0.6272, -1.6151,  0.6099],\n",
      "        [-0.2383,  1.4527, -0.7418, -0.5115, -1.0993, -1.2096,  1.7127],\n",
      "        [ 1.4735,  0.7973,  0.1058, -0.7465,  0.6852,  1.5404, -1.0098],\n",
      "        [ 0.5300, -1.8523,  1.7380, -1.2606,  0.5971,  0.1594,  0.2876],\n",
      "        [ 1.6882,  0.6892, -1.5379,  1.9792,  1.0326, -0.5124,  0.1044],\n",
      "        [-1.1462,  1.6649, -1.9827,  1.8334,  1.1945, -1.5641, -1.5750],\n",
      "        [ 1.4218, -0.3487,  1.7386,  0.0817, -0.4494,  1.5103, -1.8102]])\n",
      "Targets shape (B*T):\n",
      "torch.Size([8])\n",
      "Targets:\n",
      "tensor([4, 0, 4, 1, 4, 2, 5, 3])\n",
      "Cross-entropy Loss:\n",
      "2.834946870803833\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define parameters\n",
    "vocab_size = 7  # Number of classes (vocabulary size)\n",
    "B = 2           # Batch size\n",
    "T = 4           # Sequence length\n",
    "\n",
    "# Step 2: Create random logits (unnormalized scores)\n",
    "# Generate random values between -2 and 2 to simulate logits\n",
    "logits = torch.rand((B * T, vocab_size)) * 4 - 2\n",
    "print(\"Logits shape (B*T, vocab_size):\\n{}\".format(logits.shape))\n",
    "print(\"Logits:\\n{}\".format(logits))\n",
    "\n",
    "# Step 3: Create random targets\n",
    "# Generate random integers between 0 and vocab_size to simulate class indices\n",
    "targets = torch.randint(low=0, high=vocab_size, size=(B * T,))\n",
    "print(\"Targets shape (B*T):\\n{}\".format(targets.shape))\n",
    "print(\"Targets:\\n{}\".format(targets))\n",
    "\n",
    "# Step 4: Compute cross-entropy loss\n",
    "# Cross-entropy combines log-softmax and negative log-likelihood in one step\n",
    "loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "print(\"Cross-entropy Loss:\\n{}\".format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for the first token:\n",
      "tensor([ 0.7507, -1.6571, -1.8844,  1.9095,  1.3594,  1.9325,  0.0358])\n",
      "Probabilities for the first token:\n",
      "tensor([0.1007, 0.0091, 0.0072, 0.3207, 0.1850, 0.3281, 0.0492])\n",
      "Correct class index for the first token:\n",
      "4\n",
      "Probability of the correct class:\n",
      "0.18499410152435303\n",
      "tensor(0.1850)\n",
      "Cross-entropy loss for the first token:\n",
      "1.6874313354492188\n"
     ]
    }
   ],
   "source": [
    "first_logits = logits[0]  # Shape: (vocab_size,)\n",
    "print(\"Logits for the first token:\\n{}\".format(first_logits))\n",
    "probabilities = torch.softmax(first_logits, dim=0)\n",
    "print(\"Probabilities for the first token:\\n{}\".format(probabilities))\n",
    "correct_class = targets[0]  # The target class index for the first token\n",
    "print(\"Correct class index for the first token:\\n{}\".format(correct_class))\n",
    "correct_prob = probabilities[correct_class]\n",
    "print(\"Probability of the correct class:\\n{}\".format(correct_prob))\n",
    "\n",
    "print(correct_prob)\n",
    "first_loss = -torch.log(correct_prob)\n",
    "print(\"Cross-entropy loss for the first token:\\n{}\".format(first_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "0.20015186071395874\n",
      "Gradient of the weights:\n",
      "tensor([[-0.8948, -1.7895]])\n",
      "Initial weights:\n",
      "Parameter containing:\n",
      "tensor([[ 0.6876, -0.6148]], requires_grad=True)\n",
      "Updated weights:\n",
      "Parameter containing:\n",
      "tensor([[ 0.6966, -0.5969]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Very small example to illustrate backpropagation and gradient computation and optimizer update\n",
    "# Define a simple linear layer\n",
    "linear = torch.nn.Linear(in_features=2, out_features=1)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define the input tensor\n",
    "input = torch.tensor([[1.0, 2.0]])\n",
    "# Define the target tensor\n",
    "target = torch.tensor([[0.0]])\n",
    "\n",
    "# Forward pass\n",
    "output = linear(input)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\\n{}\".format(loss))\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "print(\"Gradient of the weights:\\n{}\".format(linear.weight.grad))\n",
    "\n",
    "# Update the weights\n",
    "# Before update\n",
    "print(\"Initial weights:\\n{}\".format(linear.weight))\n",
    "\n",
    "optimizer.step()\n",
    "print(\"Updated weights:\\n{}\".format(linear.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Inference After Linear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([1., 2., 3.])\n",
      "Unsqueezed tensor:\n",
      "tensor([[1., 2., 3.]])\n",
      "Unsqueezed tensor shape:\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Unsqueezing a tensor allows to add a new dimension to the tensor\n",
    "\n",
    "# Define a tensor of shape (3,)\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Original tensor:\\n{}\".format(tensor))\n",
    "# Unsqueeze the tensor along the first dimension\n",
    "tensor_unsqueezed = tensor.unsqueeze(0)\n",
    "print(\"Unsqueezed tensor:\\n{}\".format(tensor_unsqueezed))\n",
    "print(\"Unsqueezed tensor shape:\\n{}\".format(tensor_unsqueezed.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 7, 8, 9]])\n",
      "Tensor shape:\n",
      "torch.Size([1, 3, 4])\n",
      "Tensor:\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]]])\n",
      "Last token shape:\n",
      "torch.Size([1, 4])\n",
      "Last token:\n",
      "tensor([[ 9., 10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# Generation slices that last block size\n",
    "tensor = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "block_size = 4\n",
    "index = torch.arange(tensor.size(1)).unsqueeze(0)\n",
    "index_cond = index[:,-block_size:]\n",
    "print(index_cond)\n",
    "\n",
    "# take the last logit from a 3D tensor\n",
    "B,T,C = 1,3,4\n",
    "tensor = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]]])\n",
    "print(\"Tensor shape:\\n{}\".format(tensor.shape))\n",
    "print(\"Tensor:\\n{}\".format(tensor))\n",
    "# Select the last token\n",
    "last_token = tensor[:, -1, :]\n",
    "print(\"Last token shape:\\n{}\".format(last_token.shape))\n",
    "print(\"Last token:\\n{}\".format(last_token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Greedy token:\n",
      "0\n",
      "Sampled token:\n",
      "tensor([1])\n",
      "Scaled probs:\n",
      "tensor([9.9998e-01, 1.6935e-05, 1.6538e-08, 1.6150e-11, 1.6150e-11])\n",
      "Sampled token with temperature:\n",
      "tensor([0])\n",
      "Scaled probs:\n",
      "tensor([0.7446, 0.1550, 0.0576, 0.0214, 0.0214])\n",
      "Sampled token with temperature:\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# 3 Different ways of selecting the last token\n",
    "probs = torch.tensor([0.6, 0.2, 0.1, 0.05, 0.05])\n",
    "print(sum(probs))\n",
    "\n",
    "# 1. Greedy selection\n",
    "greedy_token = torch.argmax(probs)\n",
    "print(\"Greedy token:\\n{}\".format(greedy_token))\n",
    "\n",
    "# 2. Sampling\n",
    "sampled_token = torch.multinomial(probs, 1)\n",
    "print(\"Sampled token:\\n{}\".format(sampled_token))\n",
    "# 3. Temperature sampling\n",
    "temperature = 0.1\n",
    "# Apply temperature scaling\n",
    "scaled_probs = probs ** (1 / temperature)\n",
    "# Normalize the scaled probabilities\n",
    "scaled_probs /= scaled_probs.sum()\n",
    "# Sample from the scaled probabilities\n",
    "print(\"Scaled probs:\\n{}\".format(scaled_probs))\n",
    "sampled_token = torch.multinomial(scaled_probs, 1)\n",
    "print(\"Sampled token with temperature:\\n{}\".format(sampled_token))\n",
    "temperature = 0.7\n",
    "# Apply temperature scaling\n",
    "scaled_probs = probs ** (1 / temperature)\n",
    "# Normalize the scaled probabilities\n",
    "scaled_probs /= scaled_probs.sum()\n",
    "# Sample from the scaled probabilities\n",
    "print(\"Scaled probs:\\n{}\".format(scaled_probs))\n",
    "sampled_token = torch.multinomial(scaled_probs, 1)\n",
    "print(\"Sampled token with temperature:\\n{}\".format(sampled_token))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Additionals (exploding and vanishing gradients):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients without clipping:\n",
      "First layer gradients:\n",
      " tensor([[ 1.6520e+12,  5.4668e+11,  2.8144e+11,  6.0873e+11,  6.7473e+11,\n",
      "          1.7612e+12,  8.4165e+11,  1.7635e+12,  1.2098e+12,  1.7397e+10],\n",
      "        [-1.4486e+13, -4.7937e+12, -2.4678e+12, -5.3378e+12, -5.9165e+12,\n",
      "         -1.5443e+13, -7.3801e+12, -1.5463e+13, -1.0608e+13, -1.5255e+11],\n",
      "        [-6.1994e+12, -2.0516e+12, -1.0562e+12, -2.2844e+12, -2.5321e+12,\n",
      "         -6.6092e+12, -3.1585e+12, -6.6178e+12, -4.5401e+12, -6.5286e+10],\n",
      "        [-2.0655e+12, -6.8351e+11, -3.5188e+11, -7.6110e+11, -8.4362e+11,\n",
      "         -2.2020e+12, -1.0523e+12, -2.2049e+12, -1.5126e+12, -2.1751e+10],\n",
      "        [-5.5334e+12, -1.8312e+12, -9.4271e+11, -2.0390e+12, -2.2601e+12,\n",
      "         -5.8992e+12, -2.8192e+12, -5.9069e+12, -4.0524e+12, -5.8273e+10],\n",
      "        [ 3.2435e+12,  1.0734e+12,  5.5258e+11,  1.1952e+12,  1.3248e+12,\n",
      "          3.4579e+12,  1.6525e+12,  3.4624e+12,  2.3754e+12,  3.4157e+10],\n",
      "        [ 6.9659e+12,  2.3052e+12,  1.1868e+12,  2.5669e+12,  2.8452e+12,\n",
      "          7.4264e+12,  3.5490e+12,  7.4361e+12,  5.1015e+12,  7.3359e+10],\n",
      "        [-8.1954e+12, -2.7121e+12, -1.3962e+12, -3.0199e+12, -3.3474e+12,\n",
      "         -8.7372e+12, -4.1754e+12, -8.7486e+12, -6.0019e+12, -8.6307e+10],\n",
      "        [-4.6331e+12, -1.5332e+12, -7.8932e+11, -1.7072e+12, -1.8923e+12,\n",
      "         -4.9393e+12, -2.3605e+12, -4.9458e+12, -3.3930e+12, -4.8791e+10],\n",
      "        [-1.0483e+12, -3.4690e+11, -1.7859e+11, -3.8627e+11, -4.2815e+11,\n",
      "         -1.1176e+12, -5.3407e+11, -1.1190e+12, -7.6770e+11, -1.1039e+10]])\n",
      "Last layer gradients:\n",
      " tensor([[-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12],\n",
      "        [-2.5936e+12,  4.5613e+12,  3.6654e+12, -2.5021e+12, -3.6124e+12,\n",
      "          1.2489e+13,  8.2522e+12,  1.2122e+12, -4.4085e+12,  4.3844e+12]])\n",
      "\n",
      "Gradients after clipping:\n",
      "First layer gradients:\n",
      " tensor([[ 7.6407e-03,  2.5285e-03,  1.3017e-03,  2.8155e-03,  3.1208e-03,\n",
      "          8.1459e-03,  3.8928e-03,  8.1565e-03,  5.5957e-03,  8.0465e-05],\n",
      "        [-6.6999e-02, -2.2172e-02, -1.1414e-02, -2.4688e-02, -2.7365e-02,\n",
      "         -7.1428e-02, -3.4135e-02, -7.1521e-02, -4.9067e-02, -7.0557e-04],\n",
      "        [-2.8674e-02, -9.4889e-03, -4.8850e-03, -1.0566e-02, -1.1712e-02,\n",
      "         -3.0569e-02, -1.4609e-02, -3.0609e-02, -2.0999e-02, -3.0196e-04],\n",
      "        [-9.5532e-03, -3.1614e-03, -1.6275e-03, -3.5203e-03, -3.9019e-03,\n",
      "         -1.0185e-02, -4.8672e-03, -1.0198e-02, -6.9963e-03, -1.0061e-04],\n",
      "        [-2.5593e-02, -8.4695e-03, -4.3602e-03, -9.4308e-03, -1.0453e-02,\n",
      "         -2.7285e-02, -1.3039e-02, -2.7321e-02, -1.8743e-02, -2.6952e-04],\n",
      "        [ 1.5002e-02,  4.9645e-03,  2.5558e-03,  5.5280e-03,  6.1274e-03,\n",
      "          1.5994e-02,  7.6432e-03,  1.6014e-02,  1.0987e-02,  1.5799e-04],\n",
      "        [ 3.2219e-02,  1.0662e-02,  5.4890e-03,  1.1872e-02,  1.3160e-02,\n",
      "          3.4349e-02,  1.6415e-02,  3.4394e-02,  2.3596e-02,  3.3930e-04],\n",
      "        [-3.7906e-02, -1.2544e-02, -6.4579e-03, -1.3968e-02, -1.5482e-02,\n",
      "         -4.0412e-02, -1.9312e-02, -4.0464e-02, -2.7760e-02, -3.9919e-04],\n",
      "        [-2.1429e-02, -7.0914e-03, -3.6508e-03, -7.8963e-03, -8.7525e-03,\n",
      "         -2.2846e-02, -1.0918e-02, -2.2875e-02, -1.5694e-02, -2.2567e-04],\n",
      "        [-4.8484e-03, -1.6045e-03, -8.2601e-04, -1.7866e-03, -1.9803e-03,\n",
      "         -5.1690e-03, -2.4702e-03, -5.1757e-03, -3.5508e-03, -5.1059e-05]])\n",
      "Last layer gradients:\n",
      " tensor([[-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203],\n",
      "        [-0.0120,  0.0211,  0.0170, -0.0116, -0.0167,  0.0578,  0.0382,  0.0056,\n",
      "         -0.0204,  0.0203]])\n"
     ]
    }
   ],
   "source": [
    "# Example: Demonstrating exploding gradients with and without gradient clipping\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(10, 10) for _ in range(10)]  # 10 linear layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # No activation to amplify gradient issues\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = DeepNetwork()\n",
    "\n",
    "# Initialize weights with large values to induce exploding gradients\n",
    "for layer in model.layers:\n",
    "    nn.init.normal_(layer.weight, mean=0.0, std=10.0)\n",
    "\n",
    "# Define an input tensor\n",
    "input_tensor = torch.rand(1, 10, requires_grad=True)  # Batch size = 1, Input size = 10\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Define a simple loss function\n",
    "loss = output.sum()\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients of the first and last layer without clipping\n",
    "print(\"Gradients without clipping:\")\n",
    "print(\"First layer gradients:\\n\", model.layers[0].weight.grad)\n",
    "print(\"Last layer gradients:\\n\", model.layers[-1].weight.grad)\n",
    "\n",
    "# Apply gradient clipping\n",
    "max_norm = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "# Print gradients of the first and last layer after clipping\n",
    "print(\"\\nGradients after clipping:\")\n",
    "print(\"First layer gradients:\\n\", model.layers[0].weight.grad)\n",
    "print(\"Last layer gradients:\\n\", model.layers[-1].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with Sigmoid\n",
      "Layer 1 gradient magnitude: 1.7341980508517134e-17\n",
      "Layer 2 gradient magnitude: 8.603757675814747e-17\n",
      "Layer 3 gradient magnitude: 3.944868016401732e-16\n",
      "Layer 4 gradient magnitude: 2.8540074237969797e-15\n",
      "Layer 5 gradient magnitude: 1.3276803532177427e-14\n",
      "Layer 6 gradient magnitude: 6.872605783081465e-14\n",
      "Layer 7 gradient magnitude: 5.469681240051483e-13\n",
      "Layer 8 gradient magnitude: 4.498576424566414e-12\n",
      "Layer 9 gradient magnitude: 3.3529137799526154e-11\n",
      "Layer 10 gradient magnitude: 2.3103577384553375e-10\n",
      "Layer 11 gradient magnitude: 2.09278572249616e-09\n",
      "Layer 12 gradient magnitude: 2.1537672978411138e-08\n",
      "Layer 13 gradient magnitude: 1.6754657394812966e-07\n",
      "Layer 14 gradient magnitude: 8.938436053540499e-07\n",
      "Layer 15 gradient magnitude: 6.043735993443988e-06\n",
      "Layer 16 gradient magnitude: 3.8616293750237674e-05\n",
      "Layer 17 gradient magnitude: 0.00024742557434365153\n",
      "Layer 18 gradient magnitude: 0.0024572371039539576\n",
      "Layer 19 gradient magnitude: 0.015643732622265816\n",
      "Layer 20 gradient magnitude: 0.12140856683254242\n",
      "\n",
      "Testing with ReLU\n",
      "Layer 1 gradient magnitude: 2.1376596326394548e-10\n",
      "Layer 2 gradient magnitude: 4.686055432578939e-10\n",
      "Layer 3 gradient magnitude: 8.133968498746924e-10\n",
      "Layer 4 gradient magnitude: 3.5588392233165678e-09\n",
      "Layer 5 gradient magnitude: 6.264193608274127e-09\n",
      "Layer 6 gradient magnitude: 8.196581191555197e-09\n",
      "Layer 7 gradient magnitude: 9.433822611981668e-09\n",
      "Layer 8 gradient magnitude: 3.146315208368833e-08\n",
      "Layer 9 gradient magnitude: 1.0518968451833643e-07\n",
      "Layer 10 gradient magnitude: 1.2982087582713575e-07\n",
      "Layer 11 gradient magnitude: 6.690320901725499e-07\n",
      "Layer 12 gradient magnitude: 4.2256206143065356e-06\n",
      "Layer 13 gradient magnitude: 9.538544873066712e-06\n",
      "Layer 14 gradient magnitude: 3.6970504879718646e-05\n",
      "Layer 15 gradient magnitude: 0.00011198398715350777\n",
      "Layer 16 gradient magnitude: 0.00013882428174838424\n",
      "Layer 17 gradient magnitude: 0.0013983307871967554\n",
      "Layer 18 gradient magnitude: 0.004805316217243671\n",
      "Layer 19 gradient magnitude: 0.012805734761059284\n",
      "Layer 20 gradient magnitude: 0.03529916703701019\n"
     ]
    }
   ],
   "source": [
    "# Example: Deep network to demonstrate vanishing gradients\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=20, use_relu=False):\n",
    "        super(DeepNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(10, 10) for _ in range(num_layers)]  # Deeper network\n",
    "        )\n",
    "        self.activation = nn.ReLU() if use_relu else nn.Sigmoid()  # Choose activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))  # Apply activation after each layer\n",
    "        return x\n",
    "\n",
    "# Function to test gradients\n",
    "def test_gradients(num_layers=20, use_relu=False):\n",
    "    print(\"\\nTesting with ReLU\" if use_relu else \"\\nTesting with Sigmoid\")\n",
    "    \n",
    "    # Instantiate the network\n",
    "    model = DeepNetwork(num_layers=num_layers, use_relu=use_relu)\n",
    "\n",
    "    # Define an input tensor\n",
    "    input_tensor = torch.rand(1, 10, requires_grad=True) * 10 - 5  # Input in range [-5, 5] for sigmoid saturation\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Define a simple loss function\n",
    "    loss = output.sum()\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradient magnitudes across all layers\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        grad_magnitude = layer.weight.grad.abs().mean().item()\n",
    "        print(f\"Layer {i+1} gradient magnitude: {grad_magnitude}\")\n",
    "\n",
    "# Test with Sigmoid activation\n",
    "test_gradients(num_layers=20, use_relu=False)\n",
    "\n",
    "# Test with ReLU activation\n",
    "test_gradients(num_layers=20, use_relu=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
